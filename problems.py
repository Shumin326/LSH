# -*- coding: utf-8 -*-
"""“ESE545 Project1_turn_in.ipynb”的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MVSeTvFPgNDwPEsM3yT0JNeEo8YUezzw

# Project 1, ESE545 data mining
- Author: Shumin Yuan
- Email: shumin@seas.upenn.edu
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from numpy import vectorize
import string
from string import punctuation
from string import ascii_lowercase
import math
import timeit
# from scipy import sparse
# from scipy.sparse import csr_matrix
# from scipy.sparse import csc_matrix
import matplotlib.pyplot as plt
import csv

"""#P1: data preparing"""

def df_preprocess(df_org):
    reviewID = np.arange(len(df_org))
    stopword = ["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
    df_org['reviewText'] = df_org['reviewText'].str.lower()
    df_org['reviewText'] = df_org['reviewText'].str.replace(r'[^\w\s]',' ')
    df_org['reviewText'] = df_org['reviewText'].str.replace('_',' ')
    df_org['reviewText'] = df_org['reviewText'].str.split()
    txt_new = []
    for txt in df_org['reviewText']:
        txt_new.append([word for word in txt if word not in stopword])  #delete stopwords
    df_new = pd.DataFrame({'reviewID':reviewID,'reviewText':txt_new})
    # df_new = pd.DataFrame({'reviewID':reviewID,'reviewerID':df_org['reviewerID'],'reviewText':txt_new})
    df_new['reviewText']=[' '.join(item for item in x) for x in df_new['reviewText']]
    df_new = df_new[df_new.reviewText.str.len()>3]
    return df_new

"""#P2: K shingles
- choose k=4
- consider space and numbers
- delete reviews that is too small for k=4!!
"""

def k_shingle(df):
    #choose the number of shingles. Here k=4
    k = 4
    #consider letters, whitespace and numbers only.
    letters = ascii_lowercase+' '+'0123456789'
    #uncomment if choose k=5
    # ks =[i+j+k+m+n for i in letters for j in letters for k in letters for m in letters for n in letters]
    # all the shingles from 'aaaa' to '9999'
    ks = [i+j+k+m for i in letters for j in letters for k in letters for m in letters]
    len_ks = len(ks)
    # get a dictionary that can map a shingle to a integer. e.g. dictOfWords['aaab']=1
    dictOfWords = {ks[i]: i for i in range(len(ks))} 
    if type(df)==str:
        r = df
        r_shingle = list(dict.fromkeys([dictOfWords.get(r[i:i+k]) for i in range(len(r)-k+1)]))
        return r_shingle
    else:
    # get the shingled representation for each review text
        shingled_review_list = [list(dict.fromkeys([dictOfWords.get(txt[i:i+k]) for i in range(len(txt)-k+1)])) for txt in df.iloc[:,1]]
        return shingled_review_list

def list2matrix(shingled_review_list):
    # return a prun-broadcasted array representation for each review
    # choose prun length of 500, since less than 1/15 reviews has more than 500 shingles
    L = 500
    shingled_review_mat = np.zeros((len(shingled_review_list),L)).astype(int)
    i = 0
    for doc in shingled_review_list:
        shingled_review_mat[i] = broad_prun(doc,L)
        i+=1
    return shingled_review_mat

def broad_prun(l,L):
    # if len(l)<L, expand it by repeately add the first item
    # else, save the first L items only.
    repeated_item = l[0]
    if len(l)<L:
        l += (L-len(l))*[repeated_item]
        return l
    else:
        return l[:L]

def df_sparse_matrix(shingled_review_list):
    #not completed, need modification!
    n = len(shingled_review_list)
    ind_col = []
    row_len = []
    for shingles in shingled_review_list:
        ind_col += shingles
        row_len.append(len(shingles))
    s = max(ind_col)+1
    data = [1]*len(ind_col)
    # ind_len = [len(i) for i in shingled_review_list]
    rows = df_test.values.tolist()
    j = 0
    ind_row = []
    for i in rows:
        ind_row += [j]*i
        j += 1
    ar_dense = csr_matrix((data,(ind_row, ind_col)), shape=(len_df,s))
    return ar_dense

"""#P3: data visualization"""

def sim(d1,d2):
    # get the similarity(1-distance) of list d1 and list d2 using Jacard distance
    setA = set(d1)
    setB = set(d2)
    sim = len(setA&setB) / float(len(setA | setB))
    return sim

def data_visualization(shingled_review_list):
    ind = np.random.randint(0,157678,size=[10000,2])
    dis_set = []
    for i in range(10000):
        review_rand_1 = shingled_review_list[ind[i,0]]
        review_rand_2 = shingled_review_list[ind[i,1]]
        similarity = sim(review_rand_1,review_rand_2)
        dis = 1-similarity
        dis_set.append(dis)
    dis_set = np.array(dis_set)
    mean = np.mean(dis_set)
    min_dis = np.min(dis_set)
    print('The mean distance of random 10,000 pairs is',mean)
    print('The minimum distance of random 10,000 pairs is',min_dis)
    fig, axs = plt.subplots(1, 1, constrained_layout=True)
    axs.hist(dis_set,100)
    axs.set_title('data distribution with k=4')
    plt.show()

"""#P4: Signature matrix & bands
- m = 300, r = 5, b = 60
"""

def shingle2signature(shingled_review_mat):
    # use hash function: (a*r+b) mod R
    # here choose 300 hash functions, R = 37^4 = 1874161
    m = 300
    R = 1874161
    np.random.seed(100)
    random_a_b = np.random.randint(0,m**2,size = [m,2])
    ar_signature = np.zeros((len(shingled_review_mat),m))
    for i in range(m):
        a = random_a_b[i,0]
        b = random_a_b[i,1]
        ar_signature[:,i] = np.min((a*shingled_review_mat+b)%R,axis=1) 
        ar_signature = ar_signature.astype(int)   
    return ar_signature

def banding(ar_signature):
    m = 300
    r = 6
    b = 50    # b = m/r
    P = 1999993  #P is obtained from 'get_nearest_prime' function
    np.random.seed(200)
    coef = np.random.randint(1,P-1,size = [r,2])
    j = 0
    ar_bands = np.zeros((len(ar_signature),b))
    for v in ar_signature:
        #divide each row of signature matrix into bands
        v = v.reshape(b,r)  
        # hash a band
        sum_ = np.zeros(b)  
        for i in range(r):  
            sum_ += (coef[i,0]*v[:,i]+coef[i,1])%P
            sum_.reshape(1,-1) #return 1*n
        ar_bands[j] = sum_
        j += 1
    ar_bands = ar_bands.astype(int)
    return ar_bands

def seive(n):
    mask = np.ones(n+1)
    mask[:2] = 0
    for i in range(2, int(n**.5)+1):
        if not mask[i]:
            continue
        mask[i*i::i] = 0
    return np.argwhere(mask)
    
def get_nearest_prime(old_number):
    try:
        n = np.max(seive(2*old_number-1))
        if n < old_number+1:
            return None
        return n
    except ValueError:
        return None

# get_nearest_prime(1000000)

def m_b_r_plt():
    # choose r,b and m
    Ms = [200] # The possible value of M(the number of hash function)
    s = np.arange(0,1,0.001) # The range of similarity
    for m in Ms:
        r = [4,5,8]
        b = [m/i for i in r]
        for i in range(len(r)):        
            plt.plot(s,1-(1-s**r[i])**b[i])
        plt.vlines(0.8, 0, 1, colors = "c", linestyles = "dashed")
        plt.legend(labels = r, loc = 'best')
        plt.xlim(0.1,1)
        plt.xlabel('Sim')
        plt.ylabel('Pr(hit)')
        plt.title("# of hash function: %i" % m)
        plt.show()

"""#P5: detect pairs"""

def hash_doc(ar_bands):
    similar_docs = []
    for band in ar_bands.T:
        idx_sort = np.argsort(band)
        sorted_records_array = band[idx_sort]
        vals, idx_start, count = np.unique(sorted_records_array, return_counts=True,
                                    return_index=True)
        res = np.split(idx_sort, idx_start[1:])   
        vals = vals[count > 1]
        res = filter(lambda x: x.size > 1, res)
        similar_docs += list(res)
    return similar_docs

def final_check(similar_docs,shingled_review_list):
    # get the indexes of true pairs(dis<0.2)
    s = shingled_review_list
    buckets = similar_docs
    # expected_pairs = dict()
    pairs = dict()
    for similar_doc in buckets:
        for doc_1 in range(len(similar_doc)):
            for doc_2 in range(doc_1+1,len(similar_doc)):
                # if (similar_doc[doc_1],similar_doc[doc_2]) not in expected_pairs.keys() and (similar_doc[doc_2],similar_doc[doc_1]) not in expected_pairs.keys():
                    # expected_pairs.update({(similar_doc[doc_1],similar_doc[doc_2]):0})
                if sim(s[similar_doc[doc_1]],s[similar_doc[doc_2]])>=0.8:
                    if (similar_doc[doc_1],similar_doc[doc_2]) not in pairs.keys() and (similar_doc[doc_2],similar_doc[doc_1]) not in pairs.keys():
                        pairs.update({(similar_doc[doc_1],similar_doc[doc_2]):0})
    pairs_list = list(pairs.keys())
    # expected_pairs_list = list(expected_pairs.keys())
    # return pairs_list,expected_pairs_list
    return pairs_list

def find_pairs(df_org,out_root):
    start = timeit.default_timer()
    df_new = df_preprocess(df_org)
    stop_1 = timeit.default_timer()
    print('P1: data preprocess, time spent is',round((stop_1-start)/60,2),'min')
    shingled_review_list = k_shingle(df_new)
    stop_2 = timeit.default_timer()
    print('P2: k shingle representation, time spent is',round((stop_2-stop_1)/60,2),'min')
    data_visualization(shingled_review_list)
    stop_3 = timeit.default_timer()
    print('P3: data visualization, time spent is',round((stop_3-stop_2)/60,2),'min')
    shingled_review_mat = list2matrix(shingled_review_list)
    signature_mat = shingle2signature(shingled_review_mat)
    stop_4 = timeit.default_timer()
    print('P4: signature matrix representation, time spent is',round((stop_4-stop_3)/60,2),'min')
    banded_mat = banding(signature_mat)
    expected_similar_docs = hash_doc(banded_mat)
    pairs = final_check(expected_similar_docs,shingled_review_list)
    pair_ID_text = []
    for pair in pairs:
        d1 = pair[0]
        d2 = pair[1]
        true_id_1 = df_new.iloc[d1,0]
        true_id_2 = df_new.iloc[d2,0]
        reviewID_text_pair = (df_org.iloc[true_id_1,0],df_org.iloc[true_id_1,4],df_org.iloc[true_id_2,0],df_org.iloc[true_id_2,4])
        pair_ID_text.append(reviewID_text_pair)
    # out_root = "/content/gdrive/My Drive/ESE545/pairs_ID_text.csv"
    with open(out_root, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerows(pair_ID_text)
    stop_5 = timeit.default_timer()
    print('P5: find nearest neighbors, time spent is',round((stop_5-stop_4)/60,2),'min')
    return df_new, shingled_review_list, banded_mat

"""#P6: query"""

def find_similar_reivew(r,df_org,df_new,shingled_review_list,banded_mat):
    #delete stopwords, punctuation, etc
    start = timeit.default_timer()
    r = str_preprocess(r)
    #get the dense representation(where has '1' entry), k=4
    r_shing = k_shingle(r)
    max_sim = 0.8
    L = 500
    similar_text_ID = None
    pair = dict()
    similar_text = []
    r_mat = np.array(broad_prun(r_shing,L)).reshape(1,-1)  #shape:1*L
    r_sig = shingle2signature(r_mat)
    r_band = banding(r_sig)
    for i in range(r_band.shape[1]):
        if np.count_nonzero(banded_mat[:,i]==r_band[:,i])>0:
            reviews = np.where(banded_mat[:,i]==r_band[:,i])[0]
            for review in reviews:
                if review not in pair.keys():
                    pair.update({review:0})
    for doc_index in pair.keys():
        similarity = sim(r_shing,shingled_review_list[doc_index])
        if similarity > max_sim:
            actual_index = df_new.iloc[doc_index,0]
            similar_text_ID = df_org.iloc[actual_index,0]
            similar_text = df_org.iloc[actual_index,4]
    stop = timeit.default_timer()
    print('time spent for query: ',stop-start,'s')
    return similar_text_ID, similar_text
    # return similar_text_ID

def str_preprocess(r):
    stopword = ["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]
    r = r.lower()
    for i in string.punctuation:
        r = r.replace(i,' ')
    r = r.split()
    r = [word for word in r if word not in stopword]
    r = ' '.join(item for item in r)
    return r